---
title: "hw2"
author: "Liming Ning, Xinran Zhang"
date: "2022/2/13"
geometry: margin=1in
output:
  html_document:
    code_folding: hide
    highlight: haddock
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float: yes
urlcolor: blue
always_allow_html: true
indent: true 
fontsize: 12pt
---

```{r setup, include=F, cache=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
knitr::opts_knit$set(root.dir = 'G:/github/modern-data-mining/hw2/liming')
output_format <- ifelse(is.null(knitr::opts_knit$get("rmarkdown.pandoc.to")),
                        "text", knitr::opts_knit$get("rmarkdown.pandoc.to"))
options(scipen = 0, digits = 3)  # controls base R output
library(data.table)
library(reshape2)
library(readxl)
library(tidyverse)
library(cowplot)
library(kableExtra)
library(ggrepel)
library(skimr)
library(plotly)
library(RColorBrewer)
library(ggbiplot)
library(factoextra)
library(DescTools)
library(irlba)
library(ISLR)
library(lmtest)
library(stargazer)
library(GGally)
```




# Case Study 1: Self esteem
## Data preparation
```{r}
esteem = fread("data/NLSY79.csv")
skim(esteem)
```

```{r}
esteem.isnull = data.frame(matrix(nrow = nrow(esteem),ncol = ncol(esteem)))
for (i in 1:ncol(esteem)) {
  esteem.isnull[i] = sapply(esteem[,eval(parse(text = i))], is.null)
}
colnames(esteem.isnull) = colnames(esteem)
apply(esteem.isnull,2,sum)
```

No Null in the data set.

```{r}
apply(esteem == "", 2, sum)
```

56 subjects do not have Job05 information. We decide to exclude them.

```{r}
esteem = esteem[Income87>=0&HeightFeet05>0&Job05 != ""]
```

The data is well cleaned. There is no missing value in the data set. It is worth noting that some subjects in the data have  negative income (Income87) and negative height (HeightFeet05), which by common sense are susceptible. These numbers are erroneous and should be excluded. 30 subjects are excluded, relative small compared to the total number of subjects.

## Self esteem evaluation
### Reversion
```{r}
# for (i in c(81,87)) {
#   for (j in c(1,2,4,6,7)) {
#     esteem = esteem %>%
#   mutate_at(vars(paste0("Esteem",i,"_",j)), ~5-.)
#   }
# }

for (i in c(81,87)) {
  for (j in c(1,2,4,6,7)) {
    esteem[,paste0("Esteem",i,"_",j) := 5-get(paste0("Esteem",i,"_",j))]
  }
}
```

### Brief summary for the esteem measurements
```{r}
plot.esteem = list()
for (i in 1:10) {
  plot.esteem[[i]] = ggplot(data = esteem)+
    geom_histogram(aes(x=get(paste0("Esteem",87,"_",i))))+
    xlab(paste0("Esteem",87,"_",i))+
    theme_bw()
}

plot_grid(plotlist = plot.esteem,nrow = 4)
```

There are ten different dimensions in the evaluation of self esteem in 1987, including the perception of self ability, self value, attitude and so on. These questions can capture most of the components for self esteem. The answer to every question ranges from 1 to 4, where a larger number means a better manifestation of self esteem in this aspect. From the histogram we can see that most subjects score 3 or 4 in each question, indicating that most people have a strong self esteem. The distribution of scores is almost the same across questions, implying a positive correlation between the scores. 

### Correlation between measurements of self esteem
```{r}
corr.mat = data.frame(round(cor(esteem[,Esteem87_1:Esteem87_10]), digits = 2),var1 = names(esteem)[37:46])
corr.mat$var1 = 1:10
corr.mat.long = melt(corr.mat,id.vars = "var1", variable.name = "var2", value.name = "correlation")

my_color = brewer.pal(5, "Spectral")
ggplot(data = corr.mat.long,aes(x=as.factor(var1),y=var2,fill = correlation))+
  geom_point(aes(size = abs(correlation)), shape = 21) +
  geom_text(aes(label = correlation), size = 3, colour = "black", alpha = 0.7)+
  scale_fill_gradientn(colours = my_color) +
  xlab("var1")+
  scale_size_area(max_size = 15, guide = "none") +
  theme_bw()
```

From the correlation matrix, we can see that all ten measures of self esteem are positively correlated more or less. This is reasonable since they are all measuring self esteem and a larger nuber indicates a stronger self esteem for all of them. Despite that, their correlation is not extremely high, implying that they are effectively measuring different aspects of self esteem.

### PCA

**The first two PCs**
```{r}
pca.esteem = prcomp(esteem[,Esteem87_1:Esteem87_10], center = T)
pc.firsttwo = pca.esteem$rotation[,1:2] # the first two PCs
kbl(pc.firsttwo, caption = "Loadings for PC1 and PC2", digits = 2, booktabs = T,
    align = "lcc") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

```{r}
round(t(pc.firsttwo) %*% pc.firsttwo, digits = 3)
```

The result shows that both PCs are unit vector. They are orthogonal, judging from the fact that their inner product is zero (ignoring the rounding errors when the computer converts a decimal number into a binary number). 

**Interpretations for the first two PCs**

The first PC is essentially almost the average of all measures, evaluating the overall self esteem revealed by all questions. The second PC seems to represent the difference between question 8, 9, 10 and question 1, 2, 4. The former are all negative description of self esteem while the latter are all positive description of self esteem. Thus PC2 can be interpreted as measuring the inconsistency when people evaluate themselves facing essentially the same but only literally opposite expressions. 

**Formula for calculating PC scores**

Denote the original response vector for a subject $i$ as $y_i$, and the loading for $PC_j$ is $l_j$.

Subject $i$'s $PC_j$ score: $$y'_j = <y_i,l_j> = y^T_i l_j. $$ 

**Correlation between PC1 and PC2**

```{r}
round(cor(pca.esteem$x[,1:2]),digits = 3)
```

PC1 scores and PC2 scores are uncorrelated (ignoring the rounding errors when the computer converts a decimal number into a binary number).

**PVE**

```{r}
pve.esteem = data.table(t(summary(pca.esteem)$importance))
pve.esteem[, PC := as.factor(1:10)]
ggplot(pve.esteem,aes(x=PC,weight = `Standard deviation`))+
  geom_bar()+
  ylab("Standard deviation")+
  theme_bw()
```

The explanatory power of PCs drops sharply from PC2 on. The first PC accounts for significantly more variance than others, and the explanatory power of other PCs is quite close.

**CPVE**

```{r}
ggplot(pve.esteem,aes(x=PC,y = `Cumulative Proportion`))+
  geom_point()+
  ylab("Cumulative Proportion")+
  theme_bw()
```

60\% of the variance is explained by the first two PCs.

**Biplot**

```{r}
ggbiplot(pca.esteem,obs.scale = 1.5, var.scale = 1, varname.adjust = 2,varname.size = 3)+
  theme_bw()
```

Interpretation for PC1: the average of all questions. 

Interpretation for PC2: the difference between question 8, 9, 10 and the rest. 
### Clustering

**Determine number of clusters**

```{r}
fviz_nbclust(esteem[,Esteem87_1:Esteem87_10], kmeans, method = "wss")
```

We choose the number of clusters to be 2. 

**Cluster features**

By prior knowledge, we speculate that the two clusters are for strong self esteem group and weak self esteem group. If this is the case, the common feature should be: one group features relatively high scores in every question while the other group features relatively low scores in every question. 

```{r}
set.seed(1)
esteem.cluster2.origin = kmeans(esteem[,Esteem87_1:Esteem87_10], centers = 2)
esteem.cluster2.origin.center = t(esteem.cluster2.origin$centers)
colnames(esteem.cluster2.origin.center) = c("cluster1","cluster2")

kbl(esteem.cluster2.origin.center, caption = "Centers for Two Clusters", digits = 2, booktabs = T,
    align = "lcc") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The information of centers for the two clusters justify our speculation. The average score in each question is much smaller in the second cluster.

**Visualization**

```{r}
esteem[,cluster2.origin := as.factor(esteem.cluster2.origin$cluster)]

ggplot(data = esteem, aes(x=Esteem87_1,y=Esteem87_2,group = cluster2.origin,color=cluster2.origin))+
  geom_point()+
  geom_jitter()+
  theme_bw()
```

The boundary of two clusters is not very clear in the scatter plot of two original measurements.

```{r}
esteem.pcscore = data.table(pca.esteem$x,cluster2.origin = as.factor(esteem.cluster2.origin$cluster))

ggplot(data = esteem.pcscore,aes(x=PC1,y=PC2,group = cluster2.origin,color=cluster2.origin))+
  geom_point()+
  geom_jitter()+
  theme_bw()
```

The boundary of two clusters is very clear in the scatter plot of scores for two leading PCs. PC2 is almost not informative in identifying groups. It is roughly true that $x=0$ is the boundary. $PC1 >= 0$ belongs to the first group, and $PC1 < 0 $ belongs to the second group.

### Regression

**Prepare variables**

```{r}
pcscore.asvab = data.frame(prcomp(esteem[,16:26],scale. = T)$x) # should be scaled

esteem[,c("pc1.esteem87","pc1.asvab") := .(esteem.pcscore$PC1,pcscore.asvab$PC1)]

esteem[Income87==0,Income87 := 1] # deal with zeros

esteem.reg = esteem[,.(
  Gender,
  Education05,
  log.Income87 = log(Income87),
  Job05, # no job87
  Bmi05 = Weight05/2.20462262/((HeightFeet05+HeightInch05/12)/3.2808399)^2,
  Imagazine = as.factor(Imagazine),
  Ilibrary = as.factor(Ilibrary),
  MotherEd,
  FatherEd,
  FamilyIncome78,
  pc1.asvab,
  pc1.esteem87
)]

job.list = unique(esteem.reg[,Job05])
job.code = data.table(job05.list = job.list, Job05code = as.factor(1:length(job.list)))
esteem.reg = esteem.reg %>% 
  left_join(job.code,c("Job05" = "job05.list"))
esteem.reg[,Job05 := NULL]
```

**Full regression**
```{r}
# full regression
esteem.fullreg = lm(pc1.esteem87~.,data = esteem.reg)
summary(esteem.fullreg)
```
**only significant variables in the full regression**

```{r}
esteem.selectedreg1 = lm(pc1.esteem87~Education05+log.Income87+pc1.asvab+Job05code,data = esteem.reg)
summary(esteem.selectedreg1)
```

**Model diagnosis**

```{r}
# homoscedasticity of error
ggplot()+
  geom_point(aes(x=esteem.selectedreg1$fitted.values,y=esteem.selectedreg1$residuals))+
  xlab("fitted values")+
  ylab("residuals")+
  labs(title = "Residual plot")+
  theme_bw()
```

There is no clear trend for error. Homoscedasticity is roughly met.

```{r}
# linearity
plot.edu.esteem = ggplot(data = esteem.reg,aes(x=Education05,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "Education05 vs PC1 of Esteem87")+
  theme_bw()
plot.income.esteem = ggplot(data = esteem.reg,aes(x=log.Income87,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "logIncome87 vs PC1 of Esteem87")+
  theme_bw()
plot.asvab.esteem = ggplot(data = esteem.reg,aes(x=pc1.asvab,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "PC1 of ASVAB Score vs PC1 of Esteem87")+
  theme_bw()

plot_grid(plot.edu.esteem,plot.income.esteem,plot.asvab.esteem)
```

According to the scatter plots presented above, the linearity assumption is roughly met. Note that the prevalence of zero in Income87 makes taking log no longer the most desirable choice. Using the original value instead may be better.

```{r}
esteem.reg[,Income87 := exp(log.Income87)]
esteem.reg[Income87 == 1, Income87 := 0]

esteem.selectedreg1.new = lm(pc1.esteem87~Education05+Income87+pc1.asvab+Job05code,data = esteem.reg)
summary(esteem.selectedreg1.new)
```

```{r}
plot.edu.esteem.new = ggplot(data = esteem.reg,aes(x=Education05,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "Education05 vs PC1 of Esteem87")+
  theme_bw()
plot.income.esteem.new = ggplot(data = esteem.reg,aes(x=Income87,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "Income87 vs PC1 of Esteem87")+
  theme_bw()
plot.asvab.esteem.new = ggplot(data = esteem.reg,aes(x=pc1.asvab,y=pc1.esteem87))+
  geom_point()+
  geom_smooth()+
  labs(title = "PC1 of ASVAB Score vs PC1 of Esteem87")+
  theme_bw()

plot_grid(plot.edu.esteem.new,plot.income.esteem.new,plot.asvab.esteem.new)
```

The relationship between Income87 and Esteem seems to be more linear if we use the original scale of Income87.

**Summary**

We find that Education level in 2005, Income in 1987, PC1 of ASVAB test in 1981 and Job type in 2005 are significantly predictive to the overall self esteem measured in 1987. The predictive power of the two variables measured in 2005 cannot be interpreted as causal relationship; instead, we can regard them as having a common determinant with Esteem87 (the common cause can be the corresponding measure in 1987, because one's previous education should be positively correlated with current education, given that the person was an adult back then; so do jobs). 

Education level, Income and ASVAB test score are all positively correlated with self esteem. These are reasonable because they are all signals that show you are "better" than others in a certain field. Also, advantages in these aspects can be converted into wealth, which should be positively correlated with one's life quality (at least materially). \

Job types also significantly correlated with self esteem. Compared to Sales and Related Workers, people working as Cleaning and Building Service Workers, Health Care Technical and Support Workers, Transportation and Material Moving Workers, Entertainment Attendants and Related Workers, Physical Scientists (suspicious as there are only four observations) have significantly less self esteem, possibly because these occupations are representative for low social status (except the last one). Protective Service Workers have more self esteem, possibly because they are physically much stronger; Management Related Workers have more self esteem, which may be attributed to their relatively high social status.



# Case Study 2: Breast cancer sub-type

```{r}
cancer = fread("data/brca_subtype.csv")
patient = fread("data/brca_x_patient.csv")
```

## Summary and transformation

**Number of patients in each subtype**
```{r}
cancer[,.(number = length(A1BG)),by = BRCA_Subtype_PAM50]
```

**Histogram by subtype**

```{r}
hist.bysubtype = function(gene){
  luma = ggplot(data = cancer[BRCA_Subtype_PAM50 == "LumA"])+
    geom_histogram(aes(x=eval(parse(text = gene))))+
    xlab(gene)+
    labs(title = "LumA")+
    theme_bw()
  lumb = ggplot(data = cancer[BRCA_Subtype_PAM50 == "LumB"])+
    geom_histogram(aes(x=eval(parse(text = gene))))+
    xlab(gene)+
    labs(title = "LumB")+
    theme_bw()
  her2 = ggplot(data = cancer[BRCA_Subtype_PAM50 == "Her2"])+
    geom_histogram(aes(x=eval(parse(text = gene))))+
    xlab(gene)+
    labs(title = "Her2")+
    theme_bw()
  basal = ggplot(data = cancer[BRCA_Subtype_PAM50 == "Basal"])+
    geom_histogram(aes(x=eval(parse(text = gene))))+
    xlab(gene)+
    labs(title = "Basal")+
    theme_bw()
  plot_grid(luma,lumb,her2,basal,nrow = 2)
}

hist.bysubtype("A1BG")
hist.bysubtype("A2M")
hist.bysubtype("NAT1")
hist.bysubtype("NAT2")
hist.bysubtype("AADAC")
```

All the distributions are right-skewed. Outliers are prevalent.

**Transformation**
```{r}
# check NA
notna = !is.na(cancer)
notna.vec = apply(notna, 2, sum)
which(notna.vec==0)

# check NULL
isnull.firstrow = sapply(cancer[1],is.null)
which(isnull.firstrow == T)
```

```{r,eval=F}
# check variability
var = apply(cancer,2,var)
remove.col = which(var==0)
save(remove.col,file = "removecol.RData")
```

```{r}
load("removecol.RData") # if not saved, too slow to run
```

**Number of genes removed**

```{r}
length(remove.col)
```

A total of 278 genes needs to be removed due to zero variability.

```{r}
# remove 

cancer[,(names(remove.col)) := NULL]

# deal with zeros before transforming

get.minpositive = function(vec){
  min(vec[which(vec>0)])
}
min.positiveexp = apply(cancer[,-1], 2, get.minpositive)
```

```{r,eval=F}
for (i in 2:ncol(cancer)) {
  cancer[get(colnames(cancer)[i])==0,colnames(cancer)[i] := min.positiveexp[i-1]/2, with = F]
}
save(cancer,file = "cancer.RData")
```

```{r}
load("cancer.RData")
```

```{r}
# transforming
log.cancer = cancer[,1] %>% 
  cbind(log(cancer[,-1]))
```

## kmeans clustering

```{r}
set.seed(1)
kmeans.cancer = kmeans(log.cancer[,-1],centers = 4)
table(log.cancer$BRCA_Subtype_PAM50,kmeans.cancer$cluster)
```

Kmeans clustering only separate Basal with others. 

## Spectrum clustering

```{r}
log.cancer.scaled.centered = scale(log.cancer[,-1],center = T,scale = T)
svd.cancer.scaled = irlba(log.cancer.scaled.centered,nv = 10)

var.estimate.scaled = svd.cancer.scaled$d^2/(nrow(log.cancer.scaled.centered)-1)
pve.cancer.scaled = var.estimate.scaled/ncol(log.cancer.scaled.centered)
plot(pve.cancer.scaled,type = "b")
```

According to the elbow rule, we can use 3 PCs, as PVE decreases dramatically since PC4. 

**Comparison of scaling choices**
```{r}
pc.firsttwo.scaled = data.table(log.cancer.scaled.centered %*% svd.cancer.scaled$v[,1:2])
setnames(pc.firsttwo.scaled,c("PC1","PC2"))

log.cancer.unscaled.centered = scale(log.cancer[,-1],center = T,scale = F)
svd.cancer.unscaled = irlba(log.cancer.unscaled.centered,nv = 10)
pc.firsttwo.unscaled = data.table(log.cancer.unscaled.centered %*% svd.cancer.unscaled$v[,1:2])
setnames(pc.firsttwo.unscaled,c("PC1","PC2"))

pc.scatter.scaled = ggplot(data = pc.firsttwo.scaled,aes(x=PC1,y=PC2))+ 
  geom_point()+
  labs(title = "The First Two PCs, Scaled")+
  theme_bw()

pc.scatter.unscaled = ggplot(data = pc.firsttwo.unscaled,aes(x=PC1,y=PC2))+ 
  geom_point()+
  labs(title = "The First Two PCs, Unscaled")+
  theme_bw()

plot_grid(pc.scatter.scaled,pc.scatter.unscaled)
```

We should not scale the original data, since two clusters are obvious in the unscaled scatter plot. Intuitively we should not scale the variables, because scaling means that we assign equal weight for relative variability in every gene. This is generally not innocuous because some important genes may vary more, as they indicate the difference well; others vary a little and are not informative, thus we should not assign much weight on them by scaling. 

## Spectrum clustering, cont'd

**Optimal number of clusters**

```{r}
pc.firstfour.unscaled = data.table(log.cancer.unscaled.centered %*% svd.cancer.unscaled$v[,1:4])
setnames(pc.firstfour.unscaled,c("PC1","PC2","PC3","PC4"))
fviz_nbclust(pc.firstfour.unscaled,kmeans,method = "wss")
```

We decide to choose $k=4$. On the one hand, The total WSS is no longer decreasing dramatically after $k=4$; on the other hand, this best conforms to our prior knowledge.

**Comparison between true groups and clusters**

```{r}
set.seed(1) # otherwise the result varies
kmeans.cancer.4pc = kmeans(pc.firstfour.unscaled,centers = 4)
cancer.groupcomp = data.table(log.cancer[,1],pc.firstfour.unscaled,cluster = as.factor(kmeans.cancer.4pc$cluster))
setnames(cancer.groupcomp,1,"real.group")

ggplot(data = cancer.groupcomp,aes(x=PC1,y=PC2,color=real.group,pch=cluster))+
  geom_point()+
  theme_bw()
```

Overall, the clustering result is not bad. Basal is identified more clearly; Her2 is almost unidentified at all. The clustering is not perfect but acceptable.

**Comparison of clustering with original data and PCs**

```{r}
# original
kbl(table(log.cancer$BRCA_Subtype_PAM50,kmeans.cancer$cluster), caption = "Discrepancy Table, Original", digits = 2, booktabs = T,align = "lcccc") %>%
  kable_styling(latex_options = c("HOLD_position"))

# PC
kbl(table(log.cancer$BRCA_Subtype_PAM50,kmeans.cancer.4pc$cluster), caption = "Discrepancy Table, 4PC", digits = 2, booktabs = T,align = "lcccc") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

Judging from the discrepancy tables, PCA seldom helps in kmeans clustering. In theory PCA just makes the information more intensive; if we are using all information with the original data in kmeans, its performance should not be much worse than that using only a few PCs with partial information. If it is the case that PCA does help, it may be attributed to the exclusion of noise. 

**Classification of a new patient**

```{r,eval=F}
patient[,(names(remove.col)) := NULL]

# deal with zeros
for (i in 1:ncol(patient)) {
  patient[get(colnames(patient)[i])==0,colnames(patient)[i] := min.positiveexp[i]/2, with = F]
}
save(patient,file = "patient.RData")
```

```{r}
load("patient.RData")
```

```{r}
# transforming
log.patient = log(patient)

# centering
log.patient = log.patient - apply(log.cancer[,-1],2,mean)

patient.firsttwo.unscaled = data.table(as.matrix(log.patient) %*% svd.cancer.scaled$v[,1:2])
patient.firsttwo.unscaled.mat = rbind(patient.firsttwo.unscaled,patient.firsttwo.unscaled,patient.firsttwo.unscaled,patient.firsttwo.unscaled)

ggplot(data = cancer.groupcomp,aes(x=PC1,y=PC2,color=real.group,pch=cluster))+
  geom_point()+
  geom_point(aes(x=patient.firsttwo.unscaled$V1,y=patient.firsttwo.unscaled$V2),color = "black",size=2)+
  theme_bw()

distance = kmeans.cancer.4pc$centers[,1:2] - patient.firsttwo.unscaled.mat
distance.square = diag(as.matrix(distance) %*% t(as.matrix(distance)))

which.min(distance.square)
```

The first two PC scores of the patient is closest to the center of cluster 3. According to the discrepancy table, most observations belong to LumA. Therefore, the patient is most possibly categorized into LumA group.



# Case study 3: Auto data set

## EDA

```{r}
## Explore the data
names(Auto)
skim(Auto)
```

Everything seems to be fine: no missing values, names of variables are good. The class of each variable matches its nature.

```{r,eval=F}
# panel.hist<-function(x,...)
# {
#   usr<-par("usr");on.exit(par(usr))
#   par(usr=c(usr[1:2],0,1.5))
#   h<-hist(x,plot=FALSE)
#   breaks<-h$breaks;nB<-length(breaks)
#   y<-h$counts;y<-y/max(y)
#   rect(breaks[-nB],0,breaks[-1],y,...)
# }
# pairs(subset(Auto,select = -name),panel=panel.smooth, diag.panel=panel.hist)
```

**Correlations**

```{r}
Auto = data.table(Auto)
auto.corr.mat = data.frame(round(cor(Auto[,-"name"]), digits = 2),var1 = names(Auto)[-ncol(Auto)])
auto.corr.mat.long = data.table(melt(auto.corr.mat,id.vars = "var1", variable.name = "var2", value.name = "correlation"))
auto.corr.mat.long = auto.corr.mat.long[,var2 := as.character(var2)]
auto.corr.mat.long = auto.corr.mat.long[order(var1,var2)]

my_color = brewer.pal(5, "Spectral")
ggplot(data = auto.corr.mat.long,aes(x=var1,y=var2,fill = correlation))+
  geom_point(aes(size = abs(correlation)), shape = 21) +
  geom_text(aes(label = correlation), size = 3, colour = "black", alpha = 0.7)+
  scale_fill_gradientn(colours = my_color) +
  xlab("var1")+
  scale_size_area(max_size = 15, guide = "none") +
  theme_bw()
Auto = data.frame(Auto)
```

Some explanatory variables are highly correlated. Thus we should pay attention to possible collinearity problems.

**Pairwise Scatter Plots**

```{r}
Auto.new = select(Auto, -name)
Auto.new = Auto.new %>% 
  mutate(origin = as.factor(origin),cylinders = as.factor(cylinders))
```


```{r}
ggpairs(Auto.new)+
  theme_bw()
```

From the pairwise scatter plots, we can see that:

The original mpg is right-skewed. Also it has a lower bound of zero in common sense. Therefore, we may want to use log(mpg) instead of mpg in the following analysis.

Alternatively, `Horsepower`, `displacement` and `weight` may need a transformation; the pairwise plot tells us that mpg may be linearly correlated to their reciprocals.

Next we will examine the effects of two alternative transformations.

```{r}
Auto.new1 = Auto.new %>% 
  mutate(horsepower = 1/horsepower, displacement = 1/displacement, weight = 1/weight) %>%
  rename(c("horsepower" = "horsepower.rec", "displacement" = "displacement.rec", "weight" = "weight.rec"))
Auto.new2 = Auto.new %>% 
  mutate(mpg = log(mpg)) %>% 
  rename(c("mpg" = "log.mpg"))
```

**Transformation: displacement, horsepower and weight -> their reciprocals**

Here we have a closer look at the comparison between displacement, horsepower, weight and their reciprocals in terms of their relationship with mpg.

```{r}
ggpairs(Auto.new1)+
  theme_bw()
```

```{r message = F, eval=T}
par(mfrow=c(1,2))
plot(Auto.new$horsepower, Auto.new$mpg, pch=16) 
plot(Auto.new1$horsepower.rec, Auto.new1$mpg, pch=16)  #looks better!

par(mfrow=c(1,2))
plot(Auto.new$displacement, Auto.new$mpg, pch=16) 
plot(Auto.new1$displacement.rec, Auto.new1$mpg, pch=16)  #looks better!

par(mfrow=c(1,2))
plot(Auto.new$weight, Auto.new$mpg, pch=16) 
plot(Auto.new1$weight.rec, Auto.new1$mpg, pch=16)  #looks better!
```


The correlation between mpg and all three variables become more linear after transformation, so our following analysis will consider the reciprocals ones instead of the original ones.

**Transformation: mpg -> log(mpg)**

Here we have a closer look at the comparison between mpg and log(mpg) in terms of their relationship with the three variables.

```{r}
ggpairs(Auto.new2)+
  theme_bw()
```

```{r message = F, eval=T}
par(mfrow=c(1,2))
plot(Auto.new$horsepower, Auto.new$mpg, pch=16) 
plot(Auto.new2$horsepower, Auto.new2$log.mpg, pch=16)  #looks better!

par(mfrow=c(1,2))
plot(Auto.new$displacement, Auto.new$mpg, pch=16) 
plot(Auto.new2$displacement, Auto.new2$log.mpg, pch=16)  #looks better!

par(mfrow=c(1,2))
plot(Auto.new$weight, Auto.new$mpg, pch=16) 
plot(Auto.new2$weight, Auto.new2$log.mpg, pch=16)  #looks better!
```


The correlation between mpg and all three variables become more linear after transformation; also, when we consider log(mpg) instead of mpg, the non-linearity seems to alleviate. Our following analysis will consider log(mpg) instead of the original mpg.


## What effect does `time` have on `MPG`?

### Model 1: `MPG ~ Year`

We first do a simple regression. Let the response $y_i$ be the `MPG` and the explanatory variable $x_{i1}$ be `Year` ($i = 1, \dots, n=392$).


$$y_i = \beta_0 + \beta_1 \cdot x_{i1} + \epsilon$$

Model assumptions:

1. Linearity: given `Year`, the mean of `MPG` is described as $\textbf{E}(y_i | x_i) = \beta_0 + \beta_1 x_i$

2. Homoscedasticity: the variances of `MPG` are the same for any `Year` i.e. $\textbf{Var}(y_i | x_{i1}) = \sigma_{1}^2$

3. Normality: `MPG` is independent and normally distributed. i.e. $\epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma_{1}^2)$

4. Exogeneity: The error term is uncorrelated with the explanatory variables. 

5. Non-collinearity: none of the explanatory variables are perfectly correlated. 

We now create a model with `lm()`
```{r,results='asis'}
fit1 <- lm(mpg ~ year, data = Auto)    # model specification response ~ x1,.. 
ggplot(Auto, aes(x = year , y = mpg)) + 
  geom_point() +
  geom_smooth(aes(color = "fitted line"),method="lm",se=F) + 
  geom_hline(aes(yintercept = mean(mpg), color = "mean mpg")) +
  scale_colour_manual(name = "type",values = c("red","blue"))+
  labs(title = "Scatter Plot, year ~ mpg") +
  theme_bw()
  
stargazer::stargazer(fit1, type=output_format, align=TRUE)
```

`Year` is a significant variable at the .05 level.

`Year` effect: When model year increases by 1 unit, we expect, on average the mpg will increase about 1.2300.

### Model 2: `MPG ~ Year + Horsepower`

Let the response $y_i$ be the `MPG` and the explanatory variables be `Year` and `Horsepower` ($i = 1, \dots, n=392$).


$$y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \epsilon$$


```{r,results='asis'}
fit2 <- lm(mpg ~ year + horsepower, data = Auto) 
stargazer::stargazer(fit2, type=output_format, align=TRUE)
```

`Year` is still a significant variable at the .05 level.

`Year` effect: When model year increases by 1 unit, we expect, on average the mpg will increase about 0.657.

### Explain the difference between the two 95% CI's

The reason why the two 95% CI's differ is that, `year` is correlated with new variable `horsepower`. More specifically, they are negatively correlated according to the correlation matrix. Therefore, when `year` is the only explanatory variable, its coefficient estimate will absorb part of the effect of `horsepower` (this is an example of violation of necessary assumptions to derive unbiased estimate in regression models). In this case `horsepower` is negatively correlated with `mpg` and also negatively correlated with `year`. Therefore, when it is not included in the model, the estimate of the coefficient, as well as the confidence interval for `year`, is biased upward.

### Model 3: `MPG ~ Year * Horsepower`

```{r,results='asis'}
fit3 <- lm(mpg ~ year*horsepower, Auto)
stargazer::stargazer(fit3, type=output_format, align=TRUE)
```

```{r}
anova(fit3)
```

The interaction effect is significant at .05 level.

`Year` effect: When year increases by 1 unit, we expect, on average the mpg will increase about 2.19.

## Categorical predictors

### `cylinders` as a continuous/numeric variable

```{r,results='asis'}
fit4 <- lm(mpg ~ cylinders, data = Auto)
ggplot(Auto, aes(x = cylinders , y = mpg)) + 
  geom_point() +
  geom_smooth(aes(color = "fitted line"),method="lm",se=F) + 
  geom_hline(aes(yintercept = mean(mpg), color = "mean mpg")) +
  scale_colour_manual(name = "type",values = c("red","blue"))+
  labs(title = "Scatter Plot, year ~ mpg") +
  theme_bw()
stargazer::stargazer(fit4, type=output_format, align=TRUE)
```

Cylinders is significant at the 0.01 level; however, the correlation between cylinders and mpg is not linear.

`Cylinders` effect: When number of cylinders increases by 1, we expect, on average the mpg will decrease about -3.558.

### `cylinders` as a categorical/factor

```{r}
Auto1<-Auto
Auto1$cylinders<-as.factor(Auto1$cylinders)
levels(Auto1$cylinders)
```

```{r}
ggplot(Auto1) + geom_boxplot(aes(x = cylinders, y = mpg)) + theme_bw()
```

Our model is: 

$$y_{i|x=3} = \mu_{3} + \epsilon_i$$
$$y_{i|x=4} = \mu_{4} + \epsilon_i$$
$$y_{i|x=5} = \mu_{5} + \epsilon_i$$
$$y_{i|x=6} = \mu_{6} + \epsilon_i$$
$$y_{i|x=8} = \mu_{8} + \epsilon_i$$

where $\mu_{3}$, $\mu_{4}$, $\mu_{5}$, $\mu_{6}$, $\mu_{8}$ are the mean MPG of each level of cylinders and $\epsilon \sim \mathcal{N}(0,\sigma^2)$. We want to compare the five means and this can be done through linear model with indicator functions.

We are interested in the following hypotheses:

$$H_0: \mu_{3} = \mu_{4} = \mu_{5} = \mu_{6} = \mu_{8}$$

Let `3 cylinders` as the benchmark. The above Anova model is same as

$$y_i = \beta_{3} + \beta_{4} I\{Cylinders = 4\} + \beta_5 I\{Cylinders = 5\} + \beta_6 I\{Cylinders = 6\} + \beta_8 I\{Cylinders = 8\} + \epsilon_i$$
where
* $\beta_{3} = \textbf{E}(MPG|3)$

* $\beta_{4} = \textbf{E}(MPG|4)-\textbf{E}(MPG|3)$: the increment between 4 cylinders and 3 cylinders

* $\beta_5 = \textbf{E}(MPG|5)-\textbf{E}(MPG|3)$: the increment between 5 cylinders and 3 cylinders

* $\beta_6 = \textbf{E}(MPG|6)-\textbf{E}(MPG|3)$: the increment between 6 cylinders and 3 cylinders

* $\beta_8 = \textbf{E}(MPG|8)-\textbf{E}(MPG|3)$: the increment between 8 cylinders and 3 cylinders

To test $$H_0: \mu_{3} = \mu_{4} = \mu_{5} = \mu_{6} = \mu_{8}$$ is same as to test
$$H_0: \beta_{4} = \beta_{5} = \beta_{6} = \beta_{8} = 0$$

Now we can use `lm()` to fit the model and carry out the tests:

```{r,results='asis'}
fit5 <- lm(mpg ~ cylinders, Auto1)
stargazer::stargazer(fit5, type=output_format, align=TRUE)
```

Cylinders is significant at the .01 level.

`Cylinders` effect: We expect, on average the mpg with 3 cylinders will be 20.550, the mpg with 4 cylinders will increase 8.734 compared to the mpg with 3 cylinders, the mpg with 5 cylinders will increase 6.817 compared to the mpg with 3 cylinders, the mpg with 6 cylinders will decrease 0.577 compared to the mpg with 3 cylinders, the mpg with 8 cylinders will decrease 5.587 compared to the mpg with 3 cylinders. Autos with 4 cylinders have the largest mpg. Autos with 8 cylinders have the smallest mpg.

### Fundamental differences

The fundamental difference between the above two methods is: when we treat cylinders as a continuous variable, we assume that its correlation with mpg is linear, i.e., a certain difference in the number of cylinders predicts a certain difference in mpg, no matter it is from 5 to 6 or from 7 to 8; when we treat cylinders as a factor, we instead assume that relative to a benchmark, every specific number of cylinders predicts a specific difference in mpg. We discard the linearity assumption in the latter case; in fact, we treat them as several different binary variables.

### Test the null hypothesis

```{r}
anova(fit4,fit5)
```

We reject the null hypothesis with a p-value much smaller than 0.01. This indicates mpg relates to cylinders as a categorical variable; or equivalently, the relationship between cylinders and mpg is nonlinear.

## Results

### Describe one model: use mpg; take reciprocal of horsepower, displacement and weight

From the scatter plots presented above, we see that taking reciprocal of displacement, weight and horsepower fits the linearity assumption better. Therefore, our final model will consider the transformed ones instead of the original ones.

```{r}
Auto.new1 = data.table(Auto.new1)
Auto.new2 = data.table(Auto.new2)
```

We include explanatory variables one by one in a stepwise forward manner; when the p-value of the LR test for the new model against the model in the last step is larger than 0.05, we stop the process and get a model output.

**Forward stepwise selection**

```{r}
# stepwise forwarding
for.onestage = function(data1,data2,yname = colnames(data1)[1]){
  y = unlist(subset(data1,select = yname))
  cov = subset(data1,select = -get(yname))
  base =  lm(y~.,data = cov)
  new = list()
  lr = matrix(nrow = ncol(data2))
  for (i in 1:ncol(data2)) {
    new[[i]] = lm(y~.+data2[[i]],
                   data = cov)
    lr[i] = lrtest(base,new[[i]])[2,5]
  }
  loc = which.min(lr)
  if(lr[loc]<0.05){
    return(list(colnames(data2[,loc,with=F]),lr[loc]))
  }else{
    return('Stop searching')
  }
}

# linear terms

reg.current = Auto.new1[,.(
  mpg,
  year,
  cylinders
)] # initial covariance matrix
cov.rest = Auto.new1[,.(
  displacement.rec,
  horsepower.rec,
  weight.rec,
  acceleration,
  origin
)]

i = 0
while(i<1){
  new.va = for.onestage(reg.current,cov.rest)
  print(new.va)
  if(new.va[[1]] == 'Stop searching'){
    break
  }
  reg.current[,(new.va[[1]]) := cov.rest[,new.va[[1]],with=F]]
  cov.rest = cov.rest[,(new.va[[1]]) := NULL]
}
```

We include the reciprocal of weight, the reciprocal of horsepower, acceleration and origin sequentially, in addition to year and cylinders. Next we are going to examine the output model. 

**Final determination of models in this group**

```{r,results='asis'}
fit.prefinal = lm(mpg~.,reg.current)
stargazer(fit.prefinal,type=output_format, align=TRUE)

# try an interaction term
fit.final1<-lm(mpg~.+year:horsepower.rec,reg.current)
stargazer(fit.final1,type=output_format, align=TRUE)

fit.final2<-lm(mpg~cylinders+horsepower.rec+weight.rec+acceleration+year:horsepower.rec+origin,reg.current)
stargazer(fit.final2,type=output_format, align=TRUE)

anova(fit.final1,fit.final2) # The inclusion of year is not necessary.
```

We choose fit.final2 in this group. 

```{r}
fit.final = fit.final2
```

**Diagnosis**

```{r}
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0))
plot(fit.final, 1, pch=16) # residual plot
abline(h=0, col="blue", lwd=2)
plot(fit.final, 2) # qqplot
```

The residual plot casts some doubt on the homoscedasticity assumption. There is a trend that variance increases with the fitted values. Therefore, correcting the bias with a heteroscedasticity-robust standard error is preferred.

The QQ plots indicates that the residuals have a thicker tail than the normal distribution. 

**Prediction**

```{r}
new <- data.frame(cylinders="8",displacement.rec=1/350,horsepower.rec=1/260,weight.rec=1/4000,year=83,origin="1",acceleration = mean(Auto.new$acceleration)) # impute acceleration
CIpred <- predict(fit.final, new, interval="prediction", se.fit=TRUE)
CIpred
```

### Alternative: use log.mpg; no reciprocals

```{r}
# try to use log(mpg)
reg.current = Auto.new2[,.(
  log.mpg,
  year,
  cylinders
)] # initial covariance matrix
cov.rest = Auto.new2[,.(
  displacement,
  horsepower,
  weight,
  acceleration,
  origin
)]
```

**Forward stepwise selection**

```{r}
i = 0
while(i<1){
  new.va = for.onestage(reg.current,cov.rest)
  print(new.va)
  if(new.va[[1]] == 'Stop searching'){
    break
  }
  reg.current[,(new.va[[1]]) := cov.rest[,new.va[[1]],with=F]]
  cov.rest = cov.rest[,(new.va[[1]]) := NULL]
}
```

**Final determination of models in this group**

```{r,results='asis'}
# final model
fit.prefinal = lm(log.mpg~.,reg.current)
stargazer(fit.prefinal,type=output_format, align=TRUE)

# try an interaction term
fit.final1<-lm(log.mpg~.+year:horsepower,reg.current)
stargazer(fit.final1,type=output_format, align=TRUE)

fit.final2<-lm(log.mpg~cylinders+weight+year*horsepower+origin,reg.current)
stargazer(fit.final2,type=output_format, align=TRUE)

anova(fit.final1,fit.final2) # The inclusion of displacement is not necessary; only significant at 10%.

fit.final = fit.final2
```

**Diagnosis**

```{r}
# diagnosis

par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0))
plot(fit.final, 1, pch=16) # residual plot; the heteroscedasticity concern is well handled
abline(h=0, col="blue", lwd=2)
plot(fit.final, 2) # qqplot; less thick tail
```

### Final Decision

Finally we decide to employ the second model (the one with log(mpg)) because it solve both linearity and heteroscedasticity problems compared with the first model, as well as a larger adjusted $R^2$. The equation is: 
$$
\begin{aligned}
log.mpg = \alpha &+ \Sigma_i \beta_{1i}cylinder_i + \beta_{2}horsepower + \beta_3 weight + \Sigma_i \beta_{4i} year_i \\&+ \Sigma_i \beta_{5i} year_i \times horsepower + \Sigma_i \beta_{6i} origin_i + \epsilon.
\end{aligned}
$$

### Summary

As to cylinders, let 3 cylinders be the base, on average We expect, the mean mpg with 3 cylinders will be 20.6, the mpg with 4 cylinders will increase 2.93% compared to the mpg with 3 cylinders, the mpg with 5 cylinders will increase 3.35% compared to the mpg with 3 cylinders, the mpg with 6 cylinders will increase 2.02% compared to the mpg with 3 cylinders, the mpg with 8 cylinders will increase 26.0% compared to the mpg with 3 cylinders.

As to weight, When vehicle weight increases by 1 lbs, we expect, on average the mpg will decrease about 0.02%.

As to year, When year increases by 1 unit, we expect, on average the mpg will increase about 4.71%.

As to horsepower, When engine horsepower increases by 1 unit, we expect, on average the mpg will increase about 1.21%.

As to origin, let American be the base, on average We expect, the mean mpg with American origin will be 20, the mpg with European origin will increase 4.71% compared to the mpg with American origin, the mpg with Japanese origin will increase 6.29% compared to the mpg with American origin.

There is also some interaction effect of year and horsepower.

### Prediction

```{r}
# prediction

new <- data.frame(cylinders="8",displacement=350,horsepower=260,weight=4000,year=83,origin="1")
CIpred <- predict(fit.final, new, interval="prediction", se.fit=TRUE)
CIpred
```

When we use log(mpg) instead of original mpg, the concerns about heteroscedasticity and thicker tails are much alleviated; the linearity between the dependent variable and explanatory variables is further enhanced. Therefore, we prefer the result with log(mpg). 

The point estimate of mpg is `r exp(CIpred$fit[1])`. The 95% confidence interval in this case should be: [`r exp(CIpred$fit[2])`, `r exp(CIpred$fit[3])`]. 

# Simple regression through simulations

## Generate data

```{r}
x = seq(0,1,length = 40)
set.seed(1)
y = 1+1.2*x + rnorm(40,sd = 2)
ggplot()+
  geom_point(aes(x=x,y=y))+
  labs(title = "Scatter Plot, x vs y")+
  theme_bw()
```

## Understand the model

```{r}
sim.reg.result = lm(y~x)
summary(sim.reg.result)
```

The true values for $\beta_0$ and $\beta_1$ are 1 and 1.2, respectively, while the estimates are 1.33 and 0.91, respectively. The estimates are not very accurate.


The residual standard error is 1.79, deviating considerably from the true value $\sigma = 2$. 

```{r}
round(confint(sim.reg.result,level = 0.95),digits = 2)
```


The 95\% confidence interval for $\beta_1$ is [-1.03, 2.85]. It captures the true $\beta_1$. 

**Comparison of the true model and the estimate**

```{r}
ggplot(data.frame(x,y,fitted = sim.reg.result$fitted.values,truemean = 1+1.2*x))+
  geom_point(aes(x=x,y=y))+
  labs(title = "Scatter Plot, x vs y")+
  geom_line(aes(x=x,y=fitted,color="fitted"))+
  geom_line(aes(x=x,y=truemean,color="true mean"))+
  scale_color_manual(name="Type",values = c(`fitted`="blue",`true mean`="red"))+
  theme_bw()
```

## Diagnoses

**Residual plot**

```{r}
ggplot()+
  geom_point(aes(x=sim.reg.result$fitted.values,y=sim.reg.result$residuals))+
  xlab("fitted values")+
  ylab("residuals")+
  labs(title = "Residual plot")+
  theme_bw()
```


**QQ plot**

```{r}
ggplot()+
  geom_qq(aes(sample=sim.reg.result$residuals))+
  geom_qq_line(aes(sample=sim.reg.result$residuals))+
  xlab("Theoretical quantiles")+
  ylab("Sample quantiles")+
  labs(title = "QQ plot")+
  theme_bw()
```

**Comments**

The sample residuals seem to be uncorrelated with the fitted values, but they have thicker tails than the normal distribution when x is small, but thinner tails when x is big. This should be the result of randomization. 

## New simulation

```{r}
y.matrix = matrix(nrow = 40,ncol = 100)
result.matrix = data.frame(matrix(nrow = 100,ncol = 4))
colnames(result.matrix) = c("se","beta1","lower","upper")
t_star = qt(0.975,38)
for (i in 1:ncol(y.matrix)) {
  set.seed(i)
  y.matrix[,i] = 1+1.2*x+rnorm(40,sd=2)
  result.coef = summary(lm(y.matrix[,i]~x))$coefficients
  
  result.matrix[i,] = c(result.coef[2,2],result.coef[2,1],result.coef[2,1]-result.coef[2,2]*t_star,result.coef[2,1]+result.coef[2,2]*t_star)
}

skim(result.matrix$beta1)
```

```{r}
theory.mean = 1.2
theory.sd = 2/sqrt((var(x)*39))

comparison.table = data.frame(mean = c(mean(result.matrix$beta1),theory.mean),sd = c(sd(result.matrix$beta1),theory.sd))
rownames(comparison.table) = c("bootstraping","theory")

kbl(comparison.table, caption = "Comparison between the simulated result (n=100) and the theory result", digits = 2, booktabs = T,
    align = "lcc") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The sampling distribution is closed to the theoretical distribution, judging from the mean and standard deviation. However, there is still mild difference.

**Illustration of confidence interval**

```{r}
result.matrix = result.matrix %>% 
  mutate(number = 1:100,real.mean = rep(1.2,100))
result.matrix = result.matrix %>%
  mutate(captured = real.mean>=lower & real.mean <= upper)
sum(result.matrix$captured)
```

94\% confidence intervals capture the true beta, which is close to the prediction of the significance, 95\%.

```{r}
result.matrix.long = result.matrix %>%
  pivot_longer(c(beta1,lower,upper,real.mean),names_to = "type")


ggplot(data = result.matrix.long,aes(x=number,y=value,group=type,color=type))+
  geom_point()+
  geom_line()+
  labs(title = "Point estimates, confidence interval and real value")+
  theme_bw()
```

